\title{CS 4740 Project 2 Report}
\author{Jaclyn Huang, Dhiraj Gupta, Shannon Joyner [Kaggle Team Name: Jaclyn Huang]}
\date{\today}

\documentclass[12pt]{article}
\usepackage[a4paper, total={7in, 10in}]{geometry}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	urlcolor=blue,
}

\begin{document}
\maketitle

\section{Sequence Tagging Model}
\subsection{Implementation}
\subsubsection{Baseline}
For our baseline implementation, we stored the named entity and its label in a set. For every sentence, we checked if each named entity existed in the sentence and if so, gave the named entity that label.
\subsubsection{HMM}
For HMM, we implemented the Viterbi algorithm. We used the words in sentences as our observations and the BOI labels as our hidden observations. We used a variation of the bigram implementation we made in Project 1. We stored all probabilities in maps. For example, the emission probabilities were a two layer map. To prevent underflow, we stored all probabilities as logs. We implemented plus one smoothing for emission probabilities. To handle unknown words, we also created a backoff map. For every word in the training set, we stored label probabilities for suffixes of up to length 2. This is based on the observation that nouns tend to have similar endings like "-s", while verbs tend to have endings like "-ed".
\subsubsection{MEMM}
We implemented the MEMM similarly to the HMM with respect to algorithms and data structures. The primary difference between the HMM and MEMM comes from the MaxEnt classifier. Unlike an HMM, a MEMM predicts tags by computing the probability $P(c|x)$ with a MaxEnt classifier, where $c$ is a class (in this case a tag) and $x$ is some feature vector. The MaxEnt classifier code we used comes from {\tt nltk}, and all other code is original.

During the training process, we iterate over the data and construct features for each example sentence. Then we pass the features and labels (BIO tags) to the classifier to train. To predict tags for a sentence in the test data, we construct the feature set for a given test sentence and run the Viterbi algorithm on that sentence (the probabilities come from the trained MaxEnt classifier). We used a dictionary to represent features, since the NLTK MaxEnt classifier converts non-boolean feature dictionaries to boolean feature vectors.

Our final feature set is the following vector:
\begin{equation*}
\begin{split}
[&w_{i - 2},\ w_{i - 1},\ w_i,\ w_{i + 1}w_{i + 2},
w_{i - 1}w_i,\ w_iw_{i + 1},\ POS_{i - 2},\ POS_{i - 1},\\
&POS_i,\ w_{i + 1},\ POS_{i + 2}, \ POS_{i - 1}POS_i,\ POS_iPOS_{i + 1},\ NER_{i - 1},\\
&{\tt word\_shape,\ word\_shape\_short,\ has\_number,\ has\_uppercase,}\\
&{\tt has\_hyphen, is\_uppercase}
]
\end{split}
\end{equation*}
where $w_i$ is the $i$th word, $POS_i$ is the part-of-speech tag for $w_i$, and $NER_i$ is the NER tag for $w_i$ (consecutive symbols are bigrams). {\tt word\_shape} refers to the ``format" of a word, so something like ``Donald" would be {\tt Xxxxxx} and something like ``a2z" would be {\tt xdx}.
The intuition behind these features is that they provide a variety of information about the ``neighborhood" of a word whose tag is being predicted. Considering typical sentences and the way a human would approach the sequence tagging task, we believed that both previous and future words as well as corresponding parts of speech would help the MEMM tag correctly. Bigram features were included because named entities often span multiple words, like ``New York Police Department" or ``Donald Trump." Features like ``word shape" and {\tt has\_number} help with unknown words (since we did not use any tools like language model smoothing) and provide strong signals as to the type of entity (e.g. {\tt has\_number} likely indicates a street address or location, not a person). Many of our experiments involve the features used for the MEMM, so see the experiments section for more detail.
\subsection{Pre-Processing}
We initially applied the same pre-processing for the MEMM and the HMM. Of course, we also constructed the features described above as part of pre-processing for the MEMM. We omit the detailed explanation, but most of the above features could be computed through a single pass over a given sentence with auxiliary data structures (e.g. constructing bigrams by looking at word pairs and maintaining a dictionary of word to integer mappings). Note that we did not handle unknown words during preprocessing for the MEMM and HMM, since the unknown word features and backoff respectively take care of that. For the MEMM, the features related to words themselves were converted to boolean features, so all of them were set to false.
\subsection{Experiments}
For all of the following experiments:
\begin{itemize}
	\item We consider a label correct if and only if the entire entity is correct (so if an entity is [New York Yankees] but the system tags [New York] then it's considered incorrect).
	\item The validation set was constructed from a random sampling of 20\% of the training data, and the training set was the remaining 80\%. We only sampled once, so the validation and training sets remained fixed for all experiments.
\end{itemize}
\subsubsection{HMM}
The first experiment we ran was implementing the backoff probabilites. Originally we just implemented plus 1 smoothing and chose a small epsilon number for unknown words. However, we wanted a better model for unknown words since we know all unknown words are unlikely to have the same probabilities. So we implemented suffix probabilities with the theory that this would give a better estimation for unknown words. Suffix probabilities increased precision by 2\% and recall by 4\%.\\
Our final precision results for the Baseline and HMM model for the validation data is shown below.\\
\newline
\begin{tabular}{|l|l|l|}
\hline
& Precision & Recall \\
\hline
Baseline & 40.3\% & 26.6\%\\
\hline
HMM      & 83.6\% & 62.0\%\\
\hline
\end{tabular}
\subsubsection{MEMM}
The experiments we ran using the MEMM all involved differing feature sets, since the feature set is the key detail which affects performance of a MEMM. Our initial feature template does not contain information about structure of the words: 
\begin{equation*}
\begin{split}
[&w_{i - 2},\ w_{i - 1},\ w_i,\ w_{i + 1}w_{i + 2},
w_{i - 1}w_i,\ w_iw_{i + 1},\ POS_{i - 2},\ POS_{i - 1},\\
&POS_i,\ w_{i + 1},\ POS_{i + 2}, \ POS_{i - 1}POS_i,\ POS_iPOS_{i + 1},\ NER_{i - 1}
]
\end{split}
\end{equation*}
As a result, unknown words are not represented properly since they tend to be all zeros on the boolean features. We expected that this initial feature set would have performance comparable to our HMM, since it accounts for more information but does not handle the problem of unknown words. Examining the precision and recall for this implementation, we find the following:
\begin{center}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\textbf{Metric/Category} & {\tt LOC} & {\tt MISC} & {\tt ORG} & {\tt PER}\\
		\hline
		Precision & 0.87 & 0.83 & 0.76 & 0.81\\
		\hline
		Recall & 0.74 & 0.32 & 0.59 & 0.66\\
		\hline
		F1 Score & 0.80 & 0.47 & 0.67 & 0.73\\
		\hline
	\end{tabular}
\end{center}
We also decided to submit this system to Kaggle, where it scored 0.78566. 

To improve these results, we experimented with the template for handling unknown words as suggested in the textbook. The results for this feature template:
\begin{equation*}
\begin{split}
[&w_{i - 2},\ w_{i - 1},\ w_i,\ w_{i + 1}w_{i + 2},
w_{i - 1}w_i,\ w_iw_{i + 1},\ POS_{i - 2},\ POS_{i - 1},\\
&POS_i,\ w_{i + 1},\ POS_{i + 2}, \ POS_{i - 1}POS_i,\ POS_iPOS_{i + 1},\ NER_{i - 1},\\
&{\tt word\_shape,\ word\_shape\_short,\ has\_number,\ has\_uppercase,}\\
&{\tt has\_hyphen, is\_uppercase}
]
\end{split}
\end{equation*}
are here:
\begin{center}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\textbf{Metric/Category} & {\tt LOC} & {\tt MISC} & {\tt ORG} & {\tt PER}\\
		\hline
		Precision & 0.88 & 0.84 & 0.80 & 0.83\\
		\hline
		Recall & 0.65 & 0.21 & 0.49 & 0.55\\
		\hline
		F1 Score & 0.75 & 0.33 & 0.60 & 0.66\\
		\hline
	\end{tabular}
\end{center}
Surprisingly, the results here are slightly worse than the results from the previous step. However, this model performed better according to Kaggle. The previous model scored about 0.78, while this new model scored about 0.82. 

We noted that the new features tend to become very informative when predicting whether a word should \emph{not} be associated with a certain label. For instance, if a word does not contain an uppercase letter, then it is very unlikely to be tagged with either {\tt B-LOC}, {\tt B-MISC}, {\tt B-ORG}, or {\tt B-PER}. Indeed, all of them are ranked among the 30 most informative features (according to the coefficients learned by the logistic regression).

We suspect the discrepancy between Kaggle scores and our own experiments is because of the way we selected the validation set. Since this model has better handling for unknown words, if we stratified the validation set (as opposed to sampling randomly), we might have observed better performance from this model. For example, some of the training data is about baseball while other parts of it seem like current events. If we trained on baseball and validated on current events, this model would likely perform better than the previous model (since it would be able to identify unseen people, locations, and so on more accurately than the previous model). 

Given that these features were ranked as so informative, we also tried a feature set consisting mainly of these features: 
\begin{equation*}
\begin{split}
[
&NER_{i - 1}, {\tt word\_shape,\ word\_shape\_short,\ has\_number,\ has\_uppercase,}\\
&{\tt has\_hyphen, is\_uppercase}
]
\end{split}
\end{equation*}
The results were very poor:
\begin{center}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\textbf{Metric/Category} & {\tt LOC} & {\tt MISC} & {\tt ORG} & {\tt PER}\\
		\hline
		Precision & 0.61 & 1.0 & 1.0 & 0.00\\
		\hline
		Recall & 0.04 & 0.003 & 0.003 & 0.00\\
		\hline
		F1 Score & 0.08 & 0.01 & 0.01 & 0.00\\
		\hline
	\end{tabular}
\end{center}
The problem is likely that these features don't provide enough information to correctly classify. The context of previous words, parts of speech and the words themselves provide important clues as to whether or not a sequence of words represents an entity. This might indicate some overfitting, since these features were marked as highly informative in relation to the more expansive feature set we tried. 
\subsection{Results}
Our final system was the MEMM with feature set described in 1.1. We evaluated the system primarily using recall. We used 1-fold cross-validation to get an estimate for recall on the development set. Our development set was a random selection of about 20\% of the training data, and to keep results consistent we fixed this set after selecting it once. For performance on the test dataset we used Kaggle (since we do not have a labeled test set). Therefore, the test dataset performance was measured by the mean F1-score among the 4 tag types on half of the test set provided to us. The Kaggle score was 0.82401, which is noticeably better than the results achieved in our own experiments. We attribute this to the increased number of iterations we ran the MaxEnt classifier for to make the final submission. 

To get a sense of the kinds of errors our final tagger made, we examined the precision and recall on the development set, which was broken down by category (we included the results in). We see that {\tt MISC} was by far the worst category in part because the recall is so poor. This intuitively makes sense, since the {\tt MISC} category likely has a lot of diverse examples, and ones that haven't yet been seen by the tagger. So the probability of any given word or set of words belonging to {\tt MISC} would be quite low. In fact, examining the results as a whole we see that recall is significantly worse than precision across the board. The low recall indicates that the classifier is ``too strict" in predicting entities, and when coupled with the high precision seems to imply that the classifier makes good predictions for a subset of words or phrases that are actually entities. So one possible fix for this problem is finding a feature set which makes the classifier more liberal in predicting entity tags, rather than {\tt O}. For example, it might be better to cut out some features like POS bigrams in order to create a less strict classifier. 

\section{HMMs vs. MEMMs}
As expected, the MEMM performed better on the sequence-tagging task than the HMM. When analyzing the errors, the following patterns emerged:
\section{Competition Score}
Our Kaggle team name was ``Jaclyn Huang" and our competition score was 0.82401. Below is the image of the Kaggle submission:
\begin{center}
	\includegraphics[scale=0.25]{kaggle.png}
\end{center}
We mistakenly made other submissions under different team names but the team in this report is the correct, final one. 

\section{Individual Member Contribution}
\begin{itemize}
	\item Shannon: HMM and baseline implementation
	\item Jaclyn: MEMM implementation
	\item Dhiraj: Experiments and error analysis
\end{itemize}
\end{document}
